{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparasión de diferentes clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra para 3 conjuntos de datos artificiales bidimensionales, la forma en que se realiza la clasificación con distintos métodos. Principalmente lo hacemos para poder sacar conclusiones sobre en que situaciones un método puede ser mejor que otros, y que está haciendo internamente.\n",
    "\n",
    "Codigo obtenido de la documentación de scikit-learn, el cual se puede consultar [aquí](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# El de base\n",
    "import numpy as np\n",
    "\n",
    "# Las gráficas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Para separar los datos en entrenamiento y validación\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Para normalizar los datos (desviación estandar)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Los conjuntos artificiales típicos para probar datos\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# Los métodos de aprendizaje a utilizar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generando los 3 conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "# Y los grafiacamos para verlos\n",
    "figure = plt.figure(figsize=(30, 10))\n",
    "cm_escala = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "for (i, ds) in enumerate(datasets):\n",
    "\n",
    "    # Selecciona los valores del conjunto de datos y los escala\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Grafica\n",
    "    ax = plt.subplot(1, 3, i+1)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, cmap=cm_escala)\n",
    "    ax.set_xlim(X[:, 0].min() - .5, X[:, 0].max() + .5)\n",
    "    ax.set_ylim(X[:, 1].min() - .5, X[:, 1].max() + .5)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "figure.subplots_adjust(left=.02, right=.98)    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define la bateria de clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titulos = [u\"Vecinos próximos\", \"SVM lineal\", \"SVM gaussiano\", u\"Árbol de desición\",\n",
    "           u\"Boseques aleatórios\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "           \"Quadratic Discriminant Analysis\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generando la clasificación con cada método diferente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar por cada conjunto de datos datos de entrenamiento y prueba, y aparte vamos a clasificar todos los datos dentro del meshgrid, para asignarles colores en el fondo, que nos permitan visualizar el tipo de particiones del plano que se genera con cada uno de los métodos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Colores brillantes\n",
    "cm = plt.cm.RdBu\n",
    "cm_escala = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "for (cual, ds) in enumerate(datasets):\n",
    "    \n",
    "    print \"Base de datos \", cual\n",
    "    figure = plt.figure(figsize=(30, 30))\n",
    "\n",
    "    # Escalar y selecciona valores de entrenamiento y prueba\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    # Meshgrid para pintar las regiones\n",
    "    xx, yy = np.meshgrid(np.arange(X[:, 0].min() - .5, X[:, 0].max() + .5, 0.02),\n",
    "                         np.arange(X[:, 1].min() - .5, X[:, 1].max() + .5, 0.02))\n",
    "\n",
    "    # Por cada clasificador\n",
    "    for (i, (titulo, clf)) in enumerate(zip(titulos, classifiers)):\n",
    "        \n",
    "        # Escoge el subplot\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        \n",
    "        # El entrenamiento!!!!\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Encuentra el error de validación\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Clasifica cada punto en el meshgrid\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Asigna un color a cada punto\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Grafica los datos de entrenamiento y prueba\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_escala, s=150)\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_escala, s=150, alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(titulo, size=30)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=30, horizontalalignment='right')\n",
    "\n",
    "    figure.subplots_adjust(left=.02, right=.98)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
